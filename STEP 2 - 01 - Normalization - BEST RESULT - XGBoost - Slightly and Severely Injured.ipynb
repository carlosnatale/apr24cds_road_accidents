{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fbed441-79d8-4b7c-a49a-4e9a1d7d3627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 447670 entries, 0 to 447669\n",
      "Data columns (total 39 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   AccID                       447670 non-null  object \n",
      " 1   day                         447670 non-null  object \n",
      " 2   month                       447670 non-null  object \n",
      " 3   year                        447670 non-null  object \n",
      " 4   time                        447670 non-null  object \n",
      " 5   lum                         447670 non-null  object \n",
      " 6   atm_condition               447670 non-null  object \n",
      " 7   collision_type              447670 non-null  object \n",
      " 8   lat                         447670 non-null  float64\n",
      " 9   long                        447670 non-null  float64\n",
      " 10  route_category              447670 non-null  object \n",
      " 11  traffic_regime              447670 non-null  object \n",
      " 12  total_number_lanes          447670 non-null  object \n",
      " 13  reserved_lane_code          447670 non-null  object \n",
      " 14  longitudinal_profile        447670 non-null  object \n",
      " 15  upstream_terminal_number    447670 non-null  float64\n",
      " 16  distance_upstream_terminal  447670 non-null  float64\n",
      " 17  plan                        447670 non-null  object \n",
      " 18  surface_condition           447670 non-null  object \n",
      " 19  infra                       447670 non-null  object \n",
      " 20  accident_situation          447670 non-null  object \n",
      " 21  maximum_speed               447670 non-null  object \n",
      " 22  vehicleID                   447670 non-null  object \n",
      " 23  num_veh                     447670 non-null  object \n",
      " 24  traffic_direction           447670 non-null  object \n",
      " 25  vehicle_category            447670 non-null  object \n",
      " 26  fixed_obstacle              447670 non-null  object \n",
      " 27  mobile_obstacle             447670 non-null  object \n",
      " 28  initial_impact_point        447670 non-null  object \n",
      " 29  manv                        447670 non-null  object \n",
      " 30  motor                       447670 non-null  object \n",
      " 31  seat                        447670 non-null  object \n",
      " 32  user_category               447670 non-null  object \n",
      " 33  gravity                     447670 non-null  object \n",
      " 34  gender                      447670 non-null  object \n",
      " 35  birth_year                  447670 non-null  float64\n",
      " 36  reason_travel               447670 non-null  object \n",
      " 37  safety_equipment1           447670 non-null  object \n",
      " 38  age                         447670 non-null  float64\n",
      "dtypes: float64(6), object(33)\n",
      "memory usage: 133.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress specific future warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import the clean data\n",
    "data = pd.read_pickle('source\\data.pkl')\n",
    "\n",
    "data.info()\n",
    "\n",
    "# Copy of the original dataset for feature engineering and preprocessing\n",
    "data_processed = data.copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data_processed = data_processed.drop(['AccID', 'birth_year', 'vehicleID', 'num_veh'], axis=1)\n",
    "\n",
    "# Convert 'day', 'month', and 'time' to integers\n",
    "data_processed['day'] = data_processed['day'].astype(int)\n",
    "data_processed['month'] = data_processed['month'].astype(int)\n",
    "data_processed['time'] = data_processed['time'].astype(int)\n",
    "\n",
    "# Cyclical encoding for temporal features\n",
    "data_processed['day_sin'] = np.sin(2 * np.pi * data_processed['day'] / 31)  \n",
    "data_processed['day_cos'] = np.cos(2 * np.pi * data_processed['day'] / 31)\n",
    "\n",
    "data_processed['month_sin'] = np.sin(2 * np.pi * data_processed['month'] / 12)\n",
    "data_processed['month_cos'] = np.cos(2 * np.pi * data_processed['month'] / 12)\n",
    "\n",
    "data_processed['time_sin'] = np.sin(2 * np.pi * data_processed['time'] / 86340000) \n",
    "data_processed['time_cos'] = np.cos(2 * np.pi * data_processed['time'] / 86340000)\n",
    "\n",
    "data_processed.drop(columns=['day','month','time'],inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "data_processed.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8dda42-9364-43fb-8a2d-ed36dd76b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (358136, 211)\n",
      "Shape of X_test: (89534, 211)\n"
     ]
    }
   ],
   "source": [
    "# Selecting features and target variable\n",
    "features_dummy = ['year', 'lum', 'atm_condition', 'collision_type',\n",
    "       'route_category', 'traffic_regime', 'total_number_lanes',\n",
    "       'reserved_lane_code', 'longitudinal_profile', 'plan',\n",
    "       'surface_condition', 'infra', 'accident_situation',\n",
    "       'traffic_direction', 'vehicle_category', 'fixed_obstacle',\n",
    "       'mobile_obstacle', 'initial_impact_point', 'manv', 'motor', 'seat',\n",
    "       'user_category', 'gender', 'reason_travel',\n",
    "       'safety_equipment1']\n",
    "\n",
    "# These features will be standardized\n",
    "features_scaler = ['lat', 'long', 'upstream_terminal_number', 'distance_upstream_terminal', 'maximum_speed', 'age']\n",
    "\n",
    "# These features are between -1 and 1 and do not need any standardazations. \n",
    "features_temporal = ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'time_sin', 'time_cos']\n",
    "target = 'gravity'\n",
    "\n",
    "X = data_processed.drop(columns=[target])\n",
    "y = data_processed[target]\n",
    "y = y.astype(int)\n",
    "\n",
    "X = pd.get_dummies(X, columns=features_dummy, drop_first=True)\n",
    "\n",
    "# stratify will split the dataset according to the distribution of the classes to compensate for imbalanced datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardization: Fit only on the training data, then apply to both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train[features_scaler] = scaler.fit_transform(X_train[features_scaler])\n",
    "X_test[features_scaler] = scaler.transform(X_test[features_scaler])\n",
    "\n",
    "# Check the dimensions\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf330c-4e55-4eb1-914f-947679236b1c",
   "metadata": {},
   "source": [
    "Apply ML v1 -------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0df508-bd54-4d42-87a4-335d13f8b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import XGBoost and metrics packages for classification\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Modify the target variable 'gravity' to have two classes: \n",
    "# 1 (Severely Injured) and 0 (Slightly Injured)\n",
    "y_train = y_train.replace({1: 0, 4: 0, 2: 1, 3: 1})\n",
    "y_test = y_test.replace({1: 0, 4: 0, 2: 1, 3: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b46c35-7204-4789-8347-de28cc7843a1",
   "metadata": {},
   "source": [
    "--------> Class weights for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b58336-4dd4-4fdf-9359-28275dc48cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sd10725\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Slightly Injured       0.95      0.79      0.87     73713\n",
      "Severely Injured       0.46      0.82      0.59     15821\n",
      "\n",
      "        accuracy                           0.80     89534\n",
      "       macro avg       0.71      0.81      0.73     89534\n",
      "    weighted avg       0.87      0.80      0.82     89534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjust the scale_pos_weight parameter based on the imbalance ratio\n",
    "# Assuming Fatal (1) is the minority class\n",
    "imbalance_ratio = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "# Train the model with class weight adjustment\n",
    "xgb_model_weighted = xgb.XGBClassifier(\n",
    "    n_estimators=400, \n",
    "    max_depth=4, \n",
    "    learning_rate=0.3, \n",
    "    subsample=0.7, \n",
    "    colsample_bytree=0.8, \n",
    "    gamma=0.1, \n",
    "    random_state=42, \n",
    "    scale_pos_weight=imbalance_ratio,  # Class weight adjustment\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and classification report\n",
    "y_pred_weighted = xgb_model_weighted.predict(X_test)\n",
    "classification_report_output_weighted = classification_report(y_test, y_pred_weighted, target_names=[\"Slightly Injured\", \"Severely Injured\"])\n",
    "print(\"Class Weights Report:\")\n",
    "print(classification_report_output_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3027744-8fdd-48df-b196-f63bd8f94973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save trained XGBoost model\n",
    "with open(\"data.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(xgb_model_weighted, model_file)\n",
    "\n",
    "# Save StandardScaler\n",
    "with open(\"scaler.pkl\", \"wb\") as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4adb32-3951-443d-9ec5-e8dff78750cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importance using the already trained model\n",
    "plot_importance(xgb_model_weighted, max_num_features=15)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52a9cd-4640-44ba-bd20-ec561bd3b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure only the selected 18 features are used\n",
    "selected_features = [\n",
    "    \"day\", \"month\", \"time\", \"day_sin\", \"day_cos\", \"month_sin\", \"month_cos\", \"time_sin\", \"time_cos\", \n",
    "    \"speed_limit\", \"atm_condition\", \"collision_type\", \"lum\", \"user_category\", \"age\", \"gender\", \n",
    "    \"reason_travel\", \"safety_equipment1\"\n",
    "]\n",
    "\n",
    "# Extract only these features from X_train\n",
    "X_train_selected = X_train[selected_features]\n",
    "\n",
    "# Train and save the new scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_selected)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"Scaler re-saved with the correct 18 features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c80fd-3ff5-4b88-809f-990dfda67358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import numpy as np\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values, \n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"Slightly Injured\", \"Severely Injured\"], \n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Choose an instance to explain (e.g., the first instance in the test set)\n",
    "instance_index = 0\n",
    "instance = X_test.iloc[instance_index].values.reshape(1, -1)\n",
    "\n",
    "# Generate explanation for the chosen instance\n",
    "exp = explainer.explain_instance(\n",
    "    X_test.iloc[instance_index], \n",
    "    xgb_model_weighted.predict_proba\n",
    ")\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947455e1-b653-4d4b-967f-d41d76fc75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a SHAP explainer for XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model_weighted)\n",
    "\n",
    "# Calculate SHAP values for a subset of the test set to reduce computation time\n",
    "X_test_sample = X_test.sample(100, random_state=42)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Plot summary plot for feature importance\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=X_test.columns)\n",
    "\n",
    "# Plot SHAP values for a single prediction (e.g., the first instance in the sample)\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_test_sample.iloc[0, :], matplotlib=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d9188-ed89-4fab-83b2-7b3d6175982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the SHAP explainer using the trained model\n",
    "explainer = shap.TreeExplainer(xgb_model_weighted)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot for feature importance (global interpretability)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf6f76-807b-4428-84fd-7297633e3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_model_weighted)\n",
    "\n",
    "# Calculate SHAP values for the entire dataset (this handles multiclass automatically)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Use SHAP's internal handling to create the summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"dot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378f57e-e0f5-4e35-945d-ebed71d0072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the SHAP explainer if not already done\n",
    "explainer = shap.TreeExplainer(xgb_model_weighted)\n",
    "\n",
    "# Calculate SHAP values for the test dataset\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# List of feature names for which you'd like to display dependence plots\n",
    "features_to_plot = ['safety_equipment1_1', 'maximum_speed', 'age']  \n",
    "\n",
    "# Generate dependence plots for each feature\n",
    "for feature in features_to_plot:\n",
    "    print(f\"Dependence plot for {feature}:\")\n",
    "    shap.dependence_plot(feature, shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096210e3-d7b7-4602-941d-49c4a9842c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
