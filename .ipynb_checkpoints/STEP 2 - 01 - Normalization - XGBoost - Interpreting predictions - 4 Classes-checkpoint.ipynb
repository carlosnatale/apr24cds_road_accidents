{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c107cfc-1949-40b0-bf00-78bc975414f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 447670 entries, 0 to 447669\n",
      "Data columns (total 39 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   AccID                       447670 non-null  object \n",
      " 1   day                         447670 non-null  object \n",
      " 2   month                       447670 non-null  object \n",
      " 3   year                        447670 non-null  object \n",
      " 4   time                        447670 non-null  object \n",
      " 5   lum                         447670 non-null  object \n",
      " 6   atm_condition               447670 non-null  object \n",
      " 7   collision_type              447670 non-null  object \n",
      " 8   lat                         447670 non-null  float64\n",
      " 9   long                        447670 non-null  float64\n",
      " 10  route_category              447670 non-null  object \n",
      " 11  traffic_regime              447670 non-null  object \n",
      " 12  total_number_lanes          447670 non-null  object \n",
      " 13  reserved_lane_code          447670 non-null  object \n",
      " 14  longitudinal_profile        447670 non-null  object \n",
      " 15  upstream_terminal_number    447670 non-null  float64\n",
      " 16  distance_upstream_terminal  447670 non-null  float64\n",
      " 17  plan                        447670 non-null  object \n",
      " 18  surface_condition           447670 non-null  object \n",
      " 19  infra                       447670 non-null  object \n",
      " 20  accident_situation          447670 non-null  object \n",
      " 21  maximum_speed               447670 non-null  object \n",
      " 22  vehicleID                   447670 non-null  object \n",
      " 23  num_veh                     447670 non-null  object \n",
      " 24  traffic_direction           447670 non-null  object \n",
      " 25  vehicle_category            447670 non-null  object \n",
      " 26  fixed_obstacle              447670 non-null  object \n",
      " 27  mobile_obstacle             447670 non-null  object \n",
      " 28  initial_impact_point        447670 non-null  object \n",
      " 29  manv                        447670 non-null  object \n",
      " 30  motor                       447670 non-null  object \n",
      " 31  seat                        447670 non-null  object \n",
      " 32  user_category               447670 non-null  object \n",
      " 33  gravity                     447670 non-null  object \n",
      " 34  gender                      447670 non-null  object \n",
      " 35  birth_year                  447670 non-null  float64\n",
      " 36  reason_travel               447670 non-null  object \n",
      " 37  safety_equipment1           447670 non-null  object \n",
      " 38  age                         447670 non-null  float64\n",
      "dtypes: float64(6), object(33)\n",
      "memory usage: 133.2+ MB\n",
      "Shape of X_train: (358136, 211)\n",
      "Shape of X_test: (89534, 211)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress specific future warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import the clean data\n",
    "data = pd.read_pickle('source\\data.pkl')\n",
    "\n",
    "data.info()\n",
    "\n",
    "# Copy of the original dataset for feature engineering and preprocessing\n",
    "data_processed = data.copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data_processed = data_processed.drop(['AccID', 'birth_year', 'vehicleID', 'num_veh'], axis=1)\n",
    "\n",
    "# Convert 'day', 'month', and 'time' to integers\n",
    "data_processed['day'] = data_processed['day'].astype(int)\n",
    "data_processed['month'] = data_processed['month'].astype(int)\n",
    "data_processed['time'] = data_processed['time'].astype(int)\n",
    "\n",
    "# Cyclical encoding for temporal features\n",
    "data_processed['day_sin'] = np.sin(2 * np.pi * data_processed['day'] / 31)  \n",
    "data_processed['day_cos'] = np.cos(2 * np.pi * data_processed['day'] / 31)\n",
    "\n",
    "data_processed['month_sin'] = np.sin(2 * np.pi * data_processed['month'] / 12)\n",
    "data_processed['month_cos'] = np.cos(2 * np.pi * data_processed['month'] / 12)\n",
    "\n",
    "data_processed['time_sin'] = np.sin(2 * np.pi * data_processed['time'] / 86340000) \n",
    "data_processed['time_cos'] = np.cos(2 * np.pi * data_processed['time'] / 86340000)\n",
    "\n",
    "data_processed.drop(columns=['day','month','time'],inplace=True)\n",
    "\n",
    "# Selecting features and target variable\n",
    "features_dummy = ['year', 'lum', 'atm_condition', 'collision_type',\n",
    "       'route_category', 'traffic_regime', 'total_number_lanes',\n",
    "       'reserved_lane_code', 'longitudinal_profile', 'plan',\n",
    "       'surface_condition', 'infra', 'accident_situation',\n",
    "       'traffic_direction', 'vehicle_category', 'fixed_obstacle',\n",
    "       'mobile_obstacle', 'initial_impact_point', 'manv', 'motor', 'seat',\n",
    "       'user_category', 'gender', 'reason_travel',\n",
    "       'safety_equipment1']\n",
    "\n",
    "# These features will be standardized\n",
    "features_scaler = ['lat', 'long', 'upstream_terminal_number', 'distance_upstream_terminal', 'maximum_speed', 'age']\n",
    "\n",
    "# These features are between -1 and 1 and do not need any standardazations. \n",
    "features_temporal = ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'time_sin', 'time_cos']\n",
    "target = 'gravity'\n",
    "\n",
    "X = data_processed.drop(columns=[target])\n",
    "y = data_processed[target]\n",
    "y = y.astype(int)\n",
    "\n",
    "X = pd.get_dummies(X, columns=features_dummy, drop_first=True)\n",
    "\n",
    "# stratify will split the dataset according to the distribution of the classes to compensate for imbalanced datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardization: Fit only on the training data, then apply to both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train[features_scaler] = scaler.fit_transform(X_train[features_scaler])\n",
    "X_test[features_scaler] = scaler.transform(X_test[features_scaler])\n",
    "\n",
    "# Check the dimensions\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf330c-4e55-4eb1-914f-947679236b1c",
   "metadata": {},
   "source": [
    "Apply ML v1 -------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269f99a-7b83-4b11-8150-9f827c40aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize XGBoost classifier with default parameters\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Adjust the target variable `y` to start from 0\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Train the XGBoost model again with adjusted labels\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_adjusted = accuracy_score(y_test, y_pred)\n",
    "classification_rep_adjusted = classification_report(y_test, y_pred)\n",
    "\n",
    "print(accuracy_adjusted)\n",
    "print(classification_rep_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a314942-9fd7-44db-a2cb-1ec7734a92e2",
   "metadata": {},
   "source": [
    "Apply ML v2 -------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e0418-3db0-4849-a666-c7edd35244cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid,\n",
    "                                   n_iter=10, scoring='f1', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from the tuning process\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Use the best estimator to predict\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and classification report for the tuned model\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "tuned_classification_report = classification_report(y_test, y_pred_tuned)\n",
    "\n",
    "print(best_params)\n",
    "print(tuned_accuracy)\n",
    "print(tuned_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd2bb7-3a7c-4f9a-b65e-7cf4faaa8ea7",
   "metadata": {},
   "source": [
    "Apply ML v3--------> Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e5db2-c46a-4023-ba0c-8eb34763c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'min_child_weight': [5],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid,\n",
    "                                   n_iter=10, scoring='f1', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from the tuning process\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Use the best estimator to predict\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and classification report for the tuned model\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "tuned_classification_report = classification_report(y_test, y_pred_tuned)\n",
    "\n",
    "print(best_params)\n",
    "print(tuned_accuracy)\n",
    "print(tuned_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1c7c2-cb8e-4f88-aa81-6ae8e5734b41",
   "metadata": {},
   "source": [
    "Apply ML v4--------> SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6dbab-82a1-4598-b152-491b2ff6e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# SMOTE for oversampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Initialize the XGBoost classifier with class weights\n",
    "xgb_model_balanced = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight=class_weight_dict)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "xgb_model_balanced.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_balanced = xgb_model_balanced.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "\n",
    "report_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83461230-bac6-4b01-ac9b-bf5473a48ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6ab9e-2754-4164-8274-1898a54803c4",
   "metadata": {},
   "source": [
    "Apply ML v5--------> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cc0884-3abc-482d-9f11-0ddfa94d2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to train and evaluate the model\n",
    "def train_and_evaluate(X_train_resampled, y_train_resampled, X_test, y_test, class_weight=None):\n",
    "    # Initialize the XGBoost classifier\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    \n",
    "    # Apply class weights if specified\n",
    "    if class_weight:\n",
    "        xgb_model.set_params(scale_pos_weight=class_weight)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Generate and return the classification report\n",
    "    return classification_report(y_test, y_pred)\n",
    "\n",
    "# Original split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardization: Fit only on training data\n",
    "scaler = StandardScaler()\n",
    "X_train[features_scaler] = scaler.fit_transform(X_train[features_scaler])\n",
    "X_test[features_scaler] = scaler.transform(X_test[features_scaler])\n",
    "\n",
    "# Adjust the target variable `y` to start from 0\n",
    "y_train_adjusted = y_train - 1\n",
    "y_test_adjusted = y_test - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bc49c-6045-41ec-a60f-e647ea152fdf",
   "metadata": {},
   "source": [
    "--------> RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972fb34-e094-4793-aef6-15d409c08f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_res_rus, y_train_res_rus = rus.fit_resample(X_train, y_train_adjusted)\n",
    "report_rus = train_and_evaluate(X_train_res_rus, y_train_res_rus, X_test, y_test_adjusted)\n",
    "print(\"RandomUnderSampler Report:\\n\", report_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e0b76-79a6-4bb3-b1e3-021c22cdd8d4",
   "metadata": {},
   "source": [
    "--------> RandomOverSampler - BEST RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25e9c7-0ab1-481c-b43a-28a6bb597539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sd10725\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_res_ros, y_train_res_ros = ros.fit_resample(X_train, y_train_adjusted)\n",
    "report_ros = train_and_evaluate(X_train_res_ros, y_train_res_ros, X_test, y_test_adjusted)\n",
    "print(\"RandomOverSampler Report:\\n\", report_ros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90821482-a493-4bef-878a-3e13ec94184b",
   "metadata": {},
   "source": [
    "--------> Class Weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d06cf-d8c8-4bd6-9b4d-e8aa8a8ac7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_adjusted), y=y_train_adjusted)\n",
    "class_weight_dict = dict(zip(np.unique(y_train_adjusted), class_weights))\n",
    "report_class_weight = train_and_evaluate(X_train, y_train_adjusted, X_test, y_test_adjusted, class_weight=class_weight_dict)\n",
    "print(\"Class Weight Argument Report:\\n\", report_class_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e28e4-f0f3-443c-bbdd-2124cf7dc6d8",
   "metadata": {},
   "source": [
    "##### Apply ML v7--------> SMOTE and Undersampling Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3df2f-b2d8-476b-81d4-c49826798924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the SMOTE + RandomUnderSampler pipeline\n",
    "sampling_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),                # SMOTE to oversample minority classes\n",
    "    ('undersample', RandomUnderSampler(random_state=42))  # Undersample majority classes\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = sampling_pipeline.fit_resample(X_train, y_train_adjusted)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model_combined = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "xgb_model_combined.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_combined = xgb_model_combined.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report_combined = classification_report(y_test_adjusted, y_pred_combined)\n",
    "print(\"SMOTE + Undersampling Report:\\n\", report_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f47f2-23d2-42a2-8cc1-8c9663e895fd",
   "metadata": {},
   "source": [
    "SMOTE + Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc33113-9b82-4381-bbf1-685f5fe0f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SMOTE + Tomek Links\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled_tomek, y_train_resampled_tomek = smote_tomek.fit_resample(X_train, y_train_adjusted)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model_tomek = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_tomek.fit(X_train_resampled_tomek, y_train_resampled_tomek)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_tomek = xgb_model_tomek.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report_tomek = classification_report(y_test_adjusted, y_pred_tomek)\n",
    "print(\"SMOTE + Tomek Links Report:\\n\", report_tomek)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474629e3-e1fd-4633-aff0-5d5c903e72c1",
   "metadata": {},
   "source": [
    "SMOTE + ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d7bf0-1c3d-43cf-82a6-89a997354bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SMOTE + ENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_resampled_enn, y_train_resampled_enn = smote_enn.fit_resample(X_train, y_train_adjusted)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model_enn = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_enn.fit(X_train_resampled_enn, y_train_resampled_enn)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_enn = xgb_model_enn.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report_enn = classification_report(y_test_adjusted, y_pred_enn)\n",
    "print(\"SMOTE + ENN Report:\\n\", report_enn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfde42-3656-4c89-b664-c76a75790ac5",
   "metadata": {},
   "source": [
    "SMOTE + ENN with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f16b8-ba17-4593-9c69-69571be720a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Apply SMOTE + ENN for resampling\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train_adjusted)\n",
    "\n",
    "# Calculate class weights based on the original y_train distribution\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_adjusted), y=y_train_adjusted)\n",
    "class_weight_dict = dict(zip(np.unique(y_train_adjusted), class_weights))\n",
    "\n",
    "# Initialize XGBoost classifier with class weights\n",
    "xgb_model_combined_weighted = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42,\n",
    "                                            scale_pos_weight=class_weight_dict)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "xgb_model_combined_weighted.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_combined_weighted = xgb_model_combined_weighted.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report_combined_weighted = classification_report(y_test_adjusted, y_pred_combined_weighted)\n",
    "print(\"SMOTE + ENN with Class Weights Report:\\n\", report_combined_weighted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b6702-7d9f-4dac-9bef-6bdb0b9e6682",
   "metadata": {},
   "source": [
    " LIME explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38b222-17af-4c81-99bf-e0955113f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    mode='classification',\n",
    "    training_labels=y_train_adjusted,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=np.unique(y_train_adjusted).astype(str),\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Choose an instance to explain (e.g., the first instance in the test set)\n",
    "instance_index = 0\n",
    "instance = X_test.iloc[instance_index].values.reshape(1, -1)\n",
    "\n",
    "# Generate LIME explanation\n",
    "exp = explainer.explain_instance(instance.flatten(), xgb_model_combined_weighted.predict_proba, num_features=10)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f485fdd2-9581-4efc-bdbc-e1bd744fb3e2",
   "metadata": {},
   "source": [
    "SHAP explaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83398e7d-9e27-4440-a950-2057ad6e7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the SHAP explainer (TreeExplainer is optimized for tree-based models like XGBoost)\n",
    "explainer = shap.TreeExplainer(xgb_model_combined_weighted)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary Plot - Shows feature importance across all predictions\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Force Plot - Explains a single prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][instance_index], X_test.iloc[instance_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574468b4-d1d7-419f-a705-6ad7add109ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Initialize the LIME explainer\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values, \n",
    "    mode='classification',\n",
    "    training_labels=y_train_adjusted,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=np.unique(y_train_adjusted).astype(str),\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Explain the same instance used in SHAP\n",
    "exp = lime_explainer.explain_instance(\n",
    "    X_test.iloc[instance_index].values, \n",
    "    xgb_model_combined_weighted.predict_proba, \n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "# Display LIME explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac52848-1342-40f9-9f0c-7b1b8c257fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
