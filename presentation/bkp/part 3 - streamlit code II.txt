def part_3():
    st.header("Part 3: Modeling, Results, and Future Work")

    st.subheader("3.1 Modeling Approaches")
    st.write("Describe the algorithms used and their respective configurations:")
    st.write("- **Random Forest:** Efficient for high-dimensional data, optimized using hyperparameter tuning.")
    st.write("- **XGBoost:** Powerful gradient boosting technique, handled class imbalances effectively. Results show strong performance in recall for 'Severely Injured' cases.")
    st.write("- **AdaBoost:** Ensemble method focusing on misclassified instances.")
    st.write("- **KNN, Logistic Regression, and SVM:** Explored for specific scenarios with respective preprocessing strategies, but with varying success rates.")

    st.subheader("3.2 Evaluation Metrics")
    st.write("Key metrics used to evaluate the models:")
    st.write("- **Accuracy:** Overall correctness of predictions.")
    st.write("- **Precision, Recall, and F1-Score:** For imbalanced classes like 'Severely Injured' and 'Fatal'.")
    st.write("- **ROC-AUC:** To assess the classifier's performance in distinguishing classes.")

    st.subheader("3.3 Results")
    st.write("Summarize the outcomes of the modeling efforts:")
    st.write("- Random Forest achieved the highest F1-score of 0.91 for slightly injured cases.")
    st.write("- XGBoost excelled in recall for severely injured cases with an F1-score of 0.59 after threshold tuning.")
    st.write("- AdaBoost performed well in non-fatal classifications but struggled with precision for severe cases.")
    st.write("- KNN was effective for non-fatal cases but faced challenges with severely injured classifications.")

    # Example Graphs
    st.write("### Feature Importance - Random Forest")
    feature_importance = np.random.rand(10)  # Placeholder for real data
    features = [f"Feature {i}" for i in range(1, 11)]
    fig, ax = plt.subplots()
    ax.barh(features, feature_importance)
    ax.set_title("Feature Importance")
    st.pyplot(fig)

    st.write("### Confusion Matrix - XGBoost")
    confusion_matrix = np.array([[50, 10], [8, 32]])  # Placeholder for real data
    fig, ax = plt.subplots()
    cax = ax.matshow(confusion_matrix, cmap="coolwarm")
    plt.colorbar(cax)
    ax.set_xticklabels([''] + ['Slightly Injured', 'Severely Injured'])
    ax.set_yticklabels([''] + ['Slightly Injured', 'Severely Injured'])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    st.pyplot(fig)

    st.subheader("3.4 Strengths and Weaknesses")
    st.write("Highlight the strengths and weaknesses of the chosen approaches:")
    st.write("- **Strengths:** Effective handling of large datasets, strong recall for minority classes, and interpretable feature importance in tree-based models.")
    st.write("- **Weaknesses:** Challenges with severe class imbalance, limited precision for fatal cases, and sensitivity to dataset noise in models like SVM.")

    st.subheader("3.5 Future Work")
    st.write("Outline potential improvements and next steps:")
    st.write("- Explore advanced ensemble methods and deep learning architectures.")
    st.write("- Apply synthetic data generation techniques to address class imbalance.")
    st.write("- Investigate the impact of temporal trends using ARIMA models.")
    st.write("- Further optimize hyperparameters to improve precision for minority classes.")

if __name__ == "__main__":
    main()
