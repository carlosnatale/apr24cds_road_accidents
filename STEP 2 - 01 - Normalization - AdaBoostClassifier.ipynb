{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c107cfc-1949-40b0-bf00-78bc975414f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 447670 entries, 0 to 447669\n",
      "Data columns (total 39 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   AccID                       447670 non-null  object \n",
      " 1   day                         447670 non-null  object \n",
      " 2   month                       447670 non-null  object \n",
      " 3   year                        447670 non-null  object \n",
      " 4   time                        447670 non-null  object \n",
      " 5   lum                         447670 non-null  object \n",
      " 6   atm_condition               447670 non-null  object \n",
      " 7   collision_type              447670 non-null  object \n",
      " 8   lat                         447670 non-null  float64\n",
      " 9   long                        447670 non-null  float64\n",
      " 10  route_category              447670 non-null  object \n",
      " 11  traffic_regime              447670 non-null  object \n",
      " 12  total_number_lanes          447670 non-null  object \n",
      " 13  reserved_lane_code          447670 non-null  object \n",
      " 14  longitudinal_profile        447670 non-null  object \n",
      " 15  upstream_terminal_number    447670 non-null  float64\n",
      " 16  distance_upstream_terminal  447670 non-null  float64\n",
      " 17  plan                        447670 non-null  object \n",
      " 18  surface_condition           447670 non-null  object \n",
      " 19  infra                       447670 non-null  object \n",
      " 20  accident_situation          447670 non-null  object \n",
      " 21  maximum_speed               447670 non-null  object \n",
      " 22  vehicleID                   447670 non-null  object \n",
      " 23  num_veh                     447670 non-null  object \n",
      " 24  traffic_direction           447670 non-null  object \n",
      " 25  vehicle_category            447670 non-null  object \n",
      " 26  fixed_obstacle              447670 non-null  object \n",
      " 27  mobile_obstacle             447670 non-null  object \n",
      " 28  initial_impact_point        447670 non-null  object \n",
      " 29  manv                        447670 non-null  object \n",
      " 30  motor                       447670 non-null  object \n",
      " 31  seat                        447670 non-null  object \n",
      " 32  user_category               447670 non-null  object \n",
      " 33  gravity                     447670 non-null  object \n",
      " 34  gender                      447670 non-null  object \n",
      " 35  birth_year                  447670 non-null  float64\n",
      " 36  reason_travel               447670 non-null  object \n",
      " 37  safety_equipment1           447670 non-null  object \n",
      " 38  age                         447670 non-null  float64\n",
      "dtypes: float64(6), object(33)\n",
      "memory usage: 133.2+ MB\n",
      "Shape of X_train: (358136, 211)\n",
      "Shape of X_test: (89534, 211)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress specific future warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import the clean data\n",
    "data = pd.read_pickle('source\\data.pkl')\n",
    "\n",
    "data.info()\n",
    "\n",
    "# Copy of the original dataset for feature engineering and preprocessing\n",
    "data_processed = data.copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data_processed = data_processed.drop(['AccID', 'birth_year', 'vehicleID', 'num_veh'], axis=1)\n",
    "\n",
    "# Convert 'day', 'month', and 'time' to integers\n",
    "data_processed['day'] = data_processed['day'].astype(int)\n",
    "data_processed['month'] = data_processed['month'].astype(int)\n",
    "data_processed['time'] = data_processed['time'].astype(int)\n",
    "\n",
    "# Cyclical encoding for temporal features\n",
    "data_processed['day_sin'] = np.sin(2 * np.pi * data_processed['day'] / 31)  \n",
    "data_processed['day_cos'] = np.cos(2 * np.pi * data_processed['day'] / 31)\n",
    "\n",
    "data_processed['month_sin'] = np.sin(2 * np.pi * data_processed['month'] / 12)\n",
    "data_processed['month_cos'] = np.cos(2 * np.pi * data_processed['month'] / 12)\n",
    "\n",
    "data_processed['time_sin'] = np.sin(2 * np.pi * data_processed['time'] / 86340000) \n",
    "data_processed['time_cos'] = np.cos(2 * np.pi * data_processed['time'] / 86340000)\n",
    "\n",
    "data_processed.drop(columns=['day','month','time'],inplace=True)\n",
    "\n",
    "# Selecting features and target variable\n",
    "features_dummy = ['year', 'lum', 'atm_condition', 'collision_type',\n",
    "       'route_category', 'traffic_regime', 'total_number_lanes',\n",
    "       'reserved_lane_code', 'longitudinal_profile', 'plan',\n",
    "       'surface_condition', 'infra', 'accident_situation',\n",
    "       'traffic_direction', 'vehicle_category', 'fixed_obstacle',\n",
    "       'mobile_obstacle', 'initial_impact_point', 'manv', 'motor', 'seat',\n",
    "       'user_category', 'gender', 'reason_travel',\n",
    "       'safety_equipment1']\n",
    "\n",
    "# These features will be standardized\n",
    "features_scaler = ['lat', 'long', 'upstream_terminal_number', 'distance_upstream_terminal', 'maximum_speed', 'age']\n",
    "\n",
    "# These features are between -1 and 1 and do not need any standardazations. \n",
    "features_temporal = ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'time_sin', 'time_cos']\n",
    "target = 'gravity'\n",
    "\n",
    "X = data_processed.drop(columns=[target])\n",
    "y = data_processed[target]\n",
    "y = y.astype(int)\n",
    "\n",
    "X = pd.get_dummies(X, columns=features_dummy, drop_first=True)\n",
    "\n",
    "# stratify will split the dataset according to the distribution of the classes to compensate for imbalanced datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardization: Fit only on the training data, then apply to both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train[features_scaler] = scaler.fit_transform(X_train[features_scaler])\n",
    "X_test[features_scaler] = scaler.transform(X_test[features_scaler])\n",
    "\n",
    "# Check the dimensions\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf330c-4e55-4eb1-914f-947679236b1c",
   "metadata": {},
   "source": [
    "Apply ML v1 -------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcbbb0b-7b29-4d33-9479-f08030da600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6367413496548797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.80      0.74     37537\n",
      "           2       0.29      0.03      0.06      2256\n",
      "           3       0.50      0.36      0.42     13565\n",
      "           4       0.62      0.61      0.61     36176\n",
      "\n",
      "    accuracy                           0.64     89534\n",
      "   macro avg       0.52      0.45      0.46     89534\n",
      "weighted avg       0.62      0.64      0.62     89534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize AdaBoost Classifier\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4f7e4-5c69-4993-a5f9-f38cc8c5e14a",
   "metadata": {},
   "source": [
    "Apply ML v2 -------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ad7b5-a653-41ae-97bf-eb3bb3566238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Grid search to optimize for f1_weighted (since the dataset is likely imbalanced)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'base_estimator__max_depth': [1, 2]  \n",
    "}\n",
    "\n",
    "# Initialize a base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, class_weight='balanced')\n",
    "\n",
    "# Initialize the AdaBoost classifier with the base estimator\n",
    "ada_clf = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation, optimizing for weighted F1 score\n",
    "grid_search = GridSearchCV(estimator=ada_clf, param_grid=param_grid, scoring='f1_weighted', cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params_f1 = grid_search.best_params_\n",
    "best_score_f1 = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params_f1}\")\n",
    "print(f\"Best F1 score: {best_score_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7a8f6-df46-477e-9433-11a022235642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the AdaBoostClassifier with the best hyperparameters\n",
    "ada_clf_best = AdaBoostClassifier(n_estimators=50, learning_rate=0.01, random_state=42)\n",
    "ada_clf_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_best = ada_clf_best.predict(X_test)\n",
    "\n",
    "# Generate and display the classification report\n",
    "class_report_best = classification_report(y_test, y_pred_best)\n",
    "class_report_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61028a34-0e09-46bb-90d2-5e38c8001bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_report_best\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68819e62-e1df-4635-9a41-a591cc5888ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4aa004-02aa-42be-8f47-40de71c47595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to oversample the minority classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the AdaBoostClassifier with the best hyperparameters on the resampled dataset\n",
    "ada_clf_best = AdaBoostClassifier(n_estimators=200, learning_rate=1, random_state=42)\n",
    "ada_clf_best.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_resampled = ada_clf_best.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "class_report_resampled = classification_report(y_test, y_pred_resampled)\n",
    "print(class_report_resampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb5028-43e1-4cd6-a2fe-f3b8410583ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af2395-f9c3-43f6-acdc-957a4dcae4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Apply RandomUnderSampler to reduce the number of samples in the majority classes\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the AdaBoostClassifier with the best hyperparameters on the resampled dataset\n",
    "ada_clf_best = AdaBoostClassifier(n_estimators=200, learning_rate=1, random_state=42)\n",
    "ada_clf_best.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_resampled = ada_clf_best.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "class_report_resampled = classification_report(y_test, y_pred_resampled)\n",
    "print(class_report_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9f1a1-906d-4b07-ba16-442b22273de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb87ad3-5f99-43de-a4ed-81ce261d7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define a base classifier with class weights\n",
    "base_clf = DecisionTreeClassifier(class_weight='balanced', max_depth=1)\n",
    "\n",
    "# Use this base classifier in AdaBoost\n",
    "ada_clf_weighted = AdaBoostClassifier(base_estimator=base_clf, n_estimators=200, learning_rate=1, random_state=42)\n",
    "\n",
    "# Train on the original dataset\n",
    "ada_clf_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_weighted = ada_clf_weighted.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "class_report_weighted = classification_report(y_test, y_pred_weighted)\n",
    "print(class_report_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68003b6b-0596-46db-b311-71edec11abc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
